Array+Safe.swift
FILE: /Users/martingymer/Documents/GolfAIApp/Array+Safe.swift

extension Array {
    subscript(safe index: Index) -> Element? {
        indices.contains(index) ? self[index] : nil
    }
}



BoundingBoxView.swift
FILE: /Users/martingymer/Documents/GolfAIApp/BoundingBoxView.swift

import SwiftUI

struct BoundingBoxView: View {
    let boxes: [CGRect]       // Normalized rects (0‚Äì1, Vision-style)
    let labels: [String]
    let color: Color

    var body: some View {
        GeometryReader { geometry in
            ForEach(boxes.indices, id: \.self) { index in
                drawBox(
                    boxes[index],
                    label: labels[safe: index] ?? "",
                    in: geometry.size
                )
            }
        }
        .allowsHitTesting(false)  // Bounding boxes don‚Äôt intercept taps
        .drawingGroup()           // GPU compositing for smoother rendering
    }

    @ViewBuilder
    private func drawBox(_ box: CGRect, label: String, in size: CGSize) -> some View {
        ZStack(alignment: .topLeading) {
            Rectangle()
                .stroke(color, lineWidth: 2)
                .frame(
                    width: box.width * size.width,
                    height: box.height * size.height
                )
                .position(
                    x: box.midX * size.width,
                    y: (1 - box.midY) * size.height // Flip Y axis
                )

            Text(label)
                .font(.caption)
                .padding(2)
                .background(Color.black.opacity(0.6))
                .foregroundColor(color)
                .position(
                    x: box.midX * size.width,
                    y: (1 - box.midY) * size.height - 10
                )
        }
    }
}

// MARK: - Safe Array Access Helper






CameraPreview.swift
FILE: /Users/martingymer/Documents/GolfAIApp/CameraPreview.swift

import SwiftUI
import AVFoundation

struct CameraPreview: UIViewRepresentable {
    let session: AVCaptureSession

    func makeUIView(context: Context) -> UIView {
        let view = UIView(frame: .zero)

        let previewLayer = AVCaptureVideoPreviewLayer(session: session)
        previewLayer.videoGravity = .resizeAspectFill

        // ‚úÖ Set the preview layer‚Äôs connection rotation (iOS 17+ API)
        if let conn = previewLayer.connection,
           conn.isVideoRotationAngleSupported(0) {
            conn.videoRotationAngle = 0   // portrait
        }

        previewLayer.frame = view.bounds
        view.layer.addSublayer(previewLayer)

        // keep layer sized to view
        DispatchQueue.main.async {
            previewLayer.frame = view.bounds
        }

        return view
    }

    func updateUIView(_ uiView: UIView, context: Context) {
        if let previewLayer = uiView.layer.sublayers?.first as? AVCaptureVideoPreviewLayer {
            previewLayer.frame = uiView.bounds
        }
    }
}



CameraService.swift
FILE: /Users/martingymer/Documents/GolfAIApp/CameraService.swift

import Foundation
import AVFoundation
import Vision
import Combine
import ImageIO   // CGImagePropertyOrientation

final class CameraService: NSObject, ObservableObject, AVCaptureVideoDataOutputSampleBufferDelegate {

    // MARK: - ObservableObject
    // We don‚Äôt actually publish any state yet, but @StateObject in ContentView
    // expects an ObservableObject. Supplying the publisher satisfies the protocol.
    let objectWillChange = ObservableObjectPublisher()

    // MARK: - Capture
    private let session = AVCaptureSession()
    private let videoOutput = AVCaptureVideoDataOutput()
    private var input: AVCaptureDeviceInput!

    // simple throttle (~5 fps). Tweak or remove if you like.
    private var lastPredictionTime: Date = .distantPast
    private let frameGap: TimeInterval = 0.20

    private weak var viewModel: DetectionViewModel?

    public func getSession() -> AVCaptureSession { session }

    func start(viewModel: DetectionViewModel) {
        self.viewModel = viewModel

        guard
            let device = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back),
            let input = try? AVCaptureDeviceInput(device: device)
        else {
            print("‚ùå Could not create AVCaptureDeviceInput")
            return
        }

        self.input = input

        session.beginConfiguration()
        session.sessionPreset = .high

        if session.canAddInput(input) { session.addInput(input) }

        videoOutput.alwaysDiscardsLateVideoFrames = true
        videoOutput.videoSettings = [
            kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA
        ]
        videoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "videoQueue"))

        if session.canAddOutput(videoOutput) { session.addOutput(videoOutput) }

        // Orientation after adding the output
        if let conn = videoOutput.connection(with: .video) {
            if #available(iOS 17.0, *) {
                if conn.isVideoRotationAngleSupported(0) {
                    conn.videoRotationAngle = 0    // portrait
                }
            } else {
                if conn.isVideoOrientationSupported {
                    conn.videoOrientation = .portrait
                }
            }
        }

        session.commitConfiguration()
        startSession()
    }

    public func startSession() {
        DispatchQueue.global(qos: .userInitiated).async { self.session.startRunning() }
    }

    public func stop() {
        DispatchQueue.global(qos: .userInitiated).async {
            if self.session.isRunning { self.session.stopRunning() }
        }
    }

    // MARK: - Frame Capture
    func captureOutput(_ output: AVCaptureOutput,
                       didOutput sampleBuffer: CMSampleBuffer,
                       from connection: AVCaptureConnection) {

        // throttle
        let now = Date()
        guard now.timeIntervalSince(lastPredictionTime) >= frameGap else { return }
        lastPredictionTime = now

        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }

        // Compute EXIF orientation for Vision based on iOS version
        let exif: CGImagePropertyOrientation
        if #available(iOS 17.0, *) {
            // 17+: we have a concrete rotation angle in degrees (0, 90, 180, 270)
            exif = exifFromRotationAngle(connection.videoRotationAngle, cameraPosition: .back)
        } else {
            // 16‚àí: fall back to the old videoOrientation mapping
            exif = exifFromVideoOrientation(connection.videoOrientation, cameraPosition: .back)
        }

        // Run the model (Vision will handle resizing)
        let predictions = YOLOPredictor.shared.predictTryingCrops(pixelBuffer: pixelBuffer,exifOrientation: exif)

        if !predictions.isEmpty {
            print("Labels seen:", Set(predictions.map { $0.label }))
        }

        DispatchQueue.main.async { [weak self] in
            self?.viewModel?.update(with: predictions)
        }
    }
}

// MARK: - EXIF helpers

@available(iOS 17.0, *)
private func exifFromRotationAngle(_ angle: CGFloat,
                                   cameraPosition: AVCaptureDevice.Position) -> CGImagePropertyOrientation {
    // angle is usually 0, 90, 180, 270 (clockwise)
    switch Int(angle) % 360 {
    case 0:    // portrait
        return .right
    case 90:   // landscapeRight
        // Home/bottom on the right
        return (cameraPosition == .front) ? .down : .up
    case 180:  // portraitUpsideDown
        return .left
    case 270:  // landscapeLeft
        // Home/bottom on the left
        return (cameraPosition == .front) ? .up : .down
    default:
        return .right
    }
}

@available(iOS, introduced: 13.0, deprecated: 17.0)
private func exifFromVideoOrientation(_ vo: AVCaptureVideoOrientation,
                                      cameraPosition: AVCaptureDevice.Position) -> CGImagePropertyOrientation {
    switch vo {
    case .portrait:
        return .right
    case .portraitUpsideDown:
        return .left
    case .landscapeRight:
        // Home/bottom on the right
        return cameraPosition == .front ? .down : .up
    case .landscapeLeft:
        // Home/bottom on the left
        return cameraPosition == .front ? .up : .down
    @unknown default:
        return .right
    }
}




ContentView.swift
FILE: /Users/martingymer/Documents/GolfAIApp/ContentView.swift

import SwiftUI

struct ContentView: View {
    @StateObject var viewModel = DetectionViewModel(predictor: YOLOPredictor.shared)
    @StateObject var cameraService = CameraService()
    // ...



    var body: some View {
        ZStack {
            // Live camera preview
            CameraPreview(session: cameraService.getSession())
                .ignoresSafeArea()

            // Bounding boxes for predictions (with confidence)
            BoundingBoxView(
                boxes:  viewModel.predictions.map { $0.boundingBox },
                labels: viewModel.predictions.map {
                    let pct = Int(($0.confidence as Float) * 100)
                    return "\($0.label) \(pct)%"
                },
                color: .green
            )
            .ignoresSafeArea()

            // Debug overlay text at bottom
            VStack {
                Spacer()
                Text(viewModel.debugMessage)
                    .font(.caption)
                    .foregroundColor(.white)
                    .padding(8)
                    .background(Color.black.opacity(0.6))
                    .cornerRadius(8)
                    .padding(.bottom, 20)
            }
        }
        // üîπ Test Box overlay button (top-left)
        .overlay(
            VStack {
                HStack {
                    Button("‚ñ∂Ô∏è Test Box") {
                        let fake = Prediction(
                            label: "test",
                            confidence: 0.9,
                            boundingBox: CGRect(x: 0.45, y: 0.45, width: 0.10, height: 0.10)
                        )
                        viewModel.update(with: [fake])
                    }
                    .padding(8)
                    .background(Color.black.opacity(0.6))
                    .foregroundColor(.white)
                    .cornerRadius(8)

                    Spacer()
                }
                Spacer()
            }
            .padding(),
            alignment: .topLeading
        )
        .onAppear { cameraService.start(viewModel: viewModel) }
        .onDisappear { cameraService.stop() }
    }
}



DetectionViewModel.swift
FILE: /Users/martingymer/Documents/GolfAIApp/DetectionViewModel.swift

import Combine
import UIKit

@MainActor
final class DetectionViewModel: NSObject, ObservableObject {
    @Published var predictions: [Prediction] = []
    @Published var debugMessage: String = "Awaiting input..."

    private let predictor: Predictor
    private var lastNonEmpty = Date.distantPast
    var holdSeconds: TimeInterval = 0.7

    // No default here anymore
    init(predictor: Predictor) {
        self.predictor = predictor
        super.init()
    }

    func update(with new: [Prediction]) {
        if new.isEmpty {
            if Date().timeIntervalSince(lastNonEmpty) < holdSeconds { return }
            predictions = []
            debugMessage = "‚ö†Ô∏è No predictions"
        } else {
            lastNonEmpty = Date()
            predictions = new
            debugMessage = "‚úÖ \(new.count) object(s) detected"
        }
    }
}



deviceSupportsLiDAR.swift
FILE: /Users/martingymer/Documents/GolfAIApp/deviceSupportsLiDAR.swift

import ARKit

func deviceSupportsLiDAR() -> Bool {
    // Check if the device has a LiDAR sensor
    return ARWorldTrackingConfiguration.supportsFrameSemantics(.sceneDepth)
}



GolfAIApp.swift
FILE: /Users/martingymer/Documents/GolfAIApp/GolfAIApp.swift


import SwiftUI

@main
struct GolfAIApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()  // ‚Üê this runs live camera detection
        }
    }
}



GolfAIAppTests:DetectionViewModelTests.swift
FILE: /Users/martingymer/Documents/GolfAIApp/GolfAIAppTests:DetectionViewModelTests.swift

import XCTest
@testable import GolfAIApp

@MainActor
final class DetectionViewModelTests: XCTestCase {

    func test_update_noPredictions_setsWarning() {
        let vm = DetectionViewModel(predictor: FakePredictor(fakeResults: []))
        vm.update(with: [])
        XCTAssertEqual(vm.predictions.count, 0)
        XCTAssertTrue(vm.debugMessage.contains("No predictions"))
    }

    func test_update_onePrediction_setsSuccess() {
        let p = Prediction(label: "golf_ball", confidence: 0.91,
                           boundingBox: .init(x: 0.45, y: 0.55, width: 0.10, height: 0.10))
        let vm = DetectionViewModel(predictor: FakePredictor(fakeResults: [p]))
        vm.update(with: [p])
        XCTAssertEqual(vm.predictions.count, 1)
        XCTAssertTrue(vm.debugMessage.contains("1 object"))
    }
}



GolfAIAppTests:FakePredictor.swift
FILE: /Users/martingymer/Documents/GolfAIApp/GolfAIAppTests:FakePredictor.swift

import CoreVideo
@testable import GolfAIApp

struct FakePredictor: Predictor {
    let fakeResults: [Prediction]
    func predict(pixelBuffer: CVPixelBuffer) -> [Prediction] { fakeResults }
}



LaunchScreenView.swift
FILE: /Users/martingymer/Documents/GolfAIApp/LaunchScreenView.swift

import SwiftUI

struct LaunchScreenView: View {
    @State private var isActive = false

    var body: some View {
        Group {
            if isActive {
                ContentView()  // main app view
            } else {
                VStack(spacing: 20) {
                    Image("test_golf") // from your xcassets
                        .resizable()
                        .scaledToFit()
                        .frame(width: 150, height: 150)

                    Text("Golf Hole Detector")
                        .font(.title)
                        .bold()
                        .foregroundColor(.green)

                    ProgressView()
                        .progressViewStyle(CircularProgressViewStyle())
                }
                .frame(maxWidth: .infinity, maxHeight: .infinity)
                .background(Color.white)
                .onAppear {
                    DispatchQueue.main.asyncAfter(deadline: .now() + 2) {
                        withAnimation {
                            isActive = true
                        }
                    }
                }
            }
        }
    }
}



ModelTest.swift
FILE: /Users/martingymer/Documents/GolfAIApp/ModelTest.swift

// ModelTest.swift ‚Äì leave in project (no test target membership needed)
import SwiftUI
import UIKit

struct ModelTest: View {
    @StateObject private var viewModel: DetectionViewModel

    init() {
        _viewModel = StateObject(wrappedValue: DetectionViewModel(predictor: YOLOPredictor.shared))
    }

    var body: some View {
        VStack {
            if let img = UIImage(named: "test_sample") {
                Image(uiImage: img)
                    .resizable()
                    .scaledToFit()
                    .overlay(
                        BoundingBoxView(
                            boxes: viewModel.predictions.map { $0.boundingBox },
                            labels: viewModel.predictions.map {
                                let pct = Int(($0.confidence as Float) * 100)
                                return "\($0.label) \(pct)%"
                            },
                            color: .red
                        )
                    )
                    .onAppear {
                        let size = CGSize(width: 640, height: 640)
                        if let resized = img.resized(to: size),
                           let pb = resized.toCVPixelBuffer(size: size) {
                            
                            print("üì∏ Test image resized and converted to CVPixelBuffer ‚úÖ")
                            
                            let results = YOLOPredictor.shared.predictTryingCrops(pixelBuffer: pb, exifOrientation: .up)
                            
                            print("üß† Prediction count: \(results.count)")
                            for prediction in results {
                                print("‚Üí Label: \(prediction.label), Confidence: \(prediction.confidence), Box: \(prediction.boundingBox)")
                            }
                            
                            viewModel.update(with: results)
                        } else {
                            print("‚ö†Ô∏è Could not create CVPixelBuffer from test image")
                            viewModel.update(with: [])
                        }
                    }
            } else {
                Text("‚ùå test_sample not found")
            }
        }
        .padding()
    }
}




PixelBuffer+Resize.swift
FILE: /Users/martingymer/Documents/GolfAIApp/PixelBuffer+Resize.swift

import CoreVideo
import CoreImage
import UIKit

extension CVPixelBuffer {
    func resized(to size: CGSize) -> CVPixelBuffer? {
        let ciImage = CIImage(cvPixelBuffer: self)
        let scaleX = size.width / CGFloat(CVPixelBufferGetWidth(self))
        let scaleY = size.height / CGFloat(CVPixelBufferGetHeight(self))
        let transform = CGAffineTransform(scaleX: scaleX, y: scaleY)
        let resizedImage = ciImage.transformed(by: transform)

        let context = CIContext()
        var resizedBuffer: CVPixelBuffer?

        let attrs: [CFString: Any] = [
            kCVPixelBufferCGImageCompatibilityKey: true,
            kCVPixelBufferCGBitmapContextCompatibilityKey: true
        ]

        CVPixelBufferCreate(
            kCFAllocatorDefault,
            Int(size.width),
            Int(size.height),
            kCVPixelFormatType_32BGRA,
            attrs as CFDictionary,
            &resizedBuffer
        )

        guard let buffer = resizedBuffer else {
            print("‚ùå Failed to create resized pixel buffer")
            return nil
        }

        context.render(resizedImage, to: buffer)
        return buffer
    }
}




PredictionResult.swift
FILE: /Users/martingymer/Documents/GolfAIApp/PredictionResult.swift

import Foundation
import CoreGraphics

struct Prediction {
    let label: String
    let confidence: Float
    let boundingBox: CGRect
}



Predictor.swift
FILE: /Users/martingymer/Documents/GolfAIApp/Predictor.swift

import CoreVideo

@preconcurrency
protocol Predictor {
    func predict(pixelBuffer: CVPixelBuffer) -> [Prediction]
}



UIImageResize.swift
FILE: /Users/martingymer/Documents/GolfAIApp/UIImageResize.swift

import UIKit
import CoreVideo

extension UIImage {
    
    // Resize UIImage to target size
    func resized(to targetSize: CGSize) -> UIImage? {
        UIGraphicsBeginImageContextWithOptions(targetSize, false, 1.0)
        self.draw(in: CGRect(origin: .zero, size: targetSize))
        let resizedImage = UIGraphicsGetImageFromCurrentImageContext()
        UIGraphicsEndImageContext()
        return resizedImage
    }

    // Convert UIImage to CVPixelBuffer for ML model
    func toCVPixelBuffer(size: CGSize) -> CVPixelBuffer? {
        var pixelBuffer: CVPixelBuffer?
        let attrs: [CFString: Any] = [
            kCVPixelBufferCGImageCompatibilityKey: true,
            kCVPixelBufferCGBitmapContextCompatibilityKey: true
        ]

        let status = CVPixelBufferCreate(
            kCFAllocatorDefault,
            Int(size.width),
            Int(size.height),
            kCVPixelFormatType_32ARGB,
            attrs as CFDictionary,
            &pixelBuffer
        )

        guard status == kCVReturnSuccess, let buffer = pixelBuffer else {
            return nil
        }

        CVPixelBufferLockBaseAddress(buffer, [])
        defer { CVPixelBufferUnlockBaseAddress(buffer, []) }

        guard let context = CGContext(
            data: CVPixelBufferGetBaseAddress(buffer),
            width: Int(size.width),
            height: Int(size.height),
            bitsPerComponent: 8,
            bytesPerRow: CVPixelBufferGetBytesPerRow(buffer),
            space: CGColorSpaceCreateDeviceRGB(),
            bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue
        ) else {
            return nil
        }

        UIGraphicsPushContext(context)
        self.draw(in: CGRect(origin: .zero, size: size))
        UIGraphicsPopContext()

        return buffer
    }
}



YOLOPredictor.swift
FILE: /Users/martingymer/Documents/GolfAIApp/YOLOPredictor.swift

import Foundation
import Vision
import CoreML
import CoreVideo
import ImageIO   // CGImagePropertyOrientation

// MARK: - Predictor implementation
final class YOLOPredictor: Predictor {
    static let shared = YOLOPredictor()

    private let vnModel: VNCoreMLModel
    // Keep these generous for bring-up; we‚Äôll tighten later
    private let confidenceThreshold: VNConfidence = 0.05
    private let allowedLabels: Set<String> = [] // allow all while debugging

    private init() {
        do {
            let ml = try GolfHoleDetector(configuration: MLModelConfiguration()).model
            self.vnModel = try VNCoreMLModel(for: ml)
        } catch {
            fatalError("‚ùå Failed to load GolfHoleDetector.mlpackage: \(error)")
        }
    }

    // --- Protocol requirement (no EXIF). Reasonable default for still images.
    // You can change to predictTryingCrops if you want multi-crop by default.
    func predict(pixelBuffer: CVPixelBuffer) -> [Prediction] {
        return predict(pixelBuffer: pixelBuffer, exifOrientation: .up)
    }
}

// MARK: - Public helpers
extension YOLOPredictor {

    /// Predict with a known EXIF orientation (preferred for camera path).
    /// Uses `.scaleFit` (letterbox) which usually matches YOLO preprocessing.
    func predict(pixelBuffer: CVPixelBuffer,
                 exifOrientation: CGImagePropertyOrientation) -> [Prediction] {
        return predict(pixelBuffer: pixelBuffer,
                       exifOrientation: exifOrientation,
                       using: .scaleFit)
    }

    /// Try 3 common YOLO crop modes and stop on first that yields results.
    /// Useful while bringing the model up if you‚Äôre getting 0 detections.
    func predictTryingCrops(pixelBuffer: CVPixelBuffer,
                            exifOrientation: CGImagePropertyOrientation) -> [Prediction] {

        let order: [VNImageCropAndScaleOption] = [.scaleFit, .centerCrop, .scaleFill]
        for crop in order {
            let res = predict(pixelBuffer: pixelBuffer,
                              exifOrientation: exifOrientation,
                              using: crop)
            if !res.isEmpty {
                print("‚úÖ Using crop:", crop.rawValue)
                return res
            }
        }
        print("‚ùå No detections with any crop mode")
        return []
    }
}

// MARK: - Core Vision invocation (with crop mode)
private extension YOLOPredictor {

    func predict(pixelBuffer: CVPixelBuffer,
                 exifOrientation: CGImagePropertyOrientation,
                 using crop: VNImageCropAndScaleOption) -> [Prediction] {

        var out: [Prediction] = []

        // Build the request
        let request = VNCoreMLRequest(model: vnModel) { [confidenceThreshold, allowedLabels] req, err in
            if let err = err {
                print("‚ùå Vision error:", err)
                return
            }

            // If model doesn‚Äôt return object observations, show what we did get.
            if !(req.results is [VNRecognizedObjectObservation]) {
                let types = req.results?.map { String(describing: type(of: $0)) } ?? []
                if !types.isEmpty {
                    print("‚ÑπÔ∏è Vision returned non-object results:", types)
                }
            }

            guard let obs = req.results as? [VNRecognizedObjectObservation] else { return }
            if !(req.results is [VNRecognizedObjectObservation]) {
                let types = req.results?.map { String(describing: type(of: $0)) } ?? []
                print("‚ÑπÔ∏è Vision returned non-object results:", types)
            }

            // Optional debug: inspect first few observations
            print("üîé obsCount:", obs.count)
            for o in obs.prefix(3) {
                let tops = o.labels.prefix(3).map { "\($0.identifier)=\($0.confidence)" }
                print("  ‚Ü≥", tops.joined(separator: ", "), "box:", o.boundingBox)
            }

            for o in obs {
                let top  = o.labels.first
                let id   = top?.identifier ?? "unknown"
                let conf = top?.confidence ?? o.confidence

                guard conf >= confidenceThreshold else { continue }
                if !allowedLabels.isEmpty && !allowedLabels.contains(id) { continue }

                out.append(
                    Prediction(label: id,
                               confidence: conf,
                               boundingBox: o.boundingBox) // Vision‚Äôs normalized rect
                )
            }
        }

        // Crop/scale strategy for Vision ‚Üí CoreML
        request.imageCropAndScaleOption = crop

        // Run the request
        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer,
                                            orientation: exifOrientation,
                                            options: [:])
        do { try handler.perform([request]) }
        catch { print("‚ùå Vision perform error:", error) }

        return out
    }
}




