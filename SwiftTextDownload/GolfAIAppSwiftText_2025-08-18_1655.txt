Array+Safe.swift
FILE: /Users/martingymer/Documents/GolfAIApp/Array+Safe.swift

extension Array {
    subscript(safe index: Index) -> Element? {
        indices.contains(index) ? self[index] : nil
    }
}



BoundingBoxView.swift
FILE: /Users/martingymer/Documents/GolfAIApp/BoundingBoxView.swift

import SwiftUI

struct BoundingBoxView: View {
    let boxes: [CGRect]       // Normalized rects (0‚Äì1, Vision-style)
    let labels: [String]
    let color: Color

    var body: some View {
        GeometryReader { geometry in
            ForEach(boxes.indices, id: \.self) { index in
                drawBox(
                    boxes[index],
                    label: labels[safe: index] ?? "",
                    in: geometry.size
                )
            }
        }
        .allowsHitTesting(false)  // Bounding boxes don‚Äôt intercept taps
        .drawingGroup()           // GPU compositing for smoother rendering
    }

    @ViewBuilder
    private func drawBox(_ box: CGRect, label: String, in size: CGSize) -> some View {
        ZStack(alignment: .topLeading) {
            Rectangle()
                .stroke(color, lineWidth: 2)
                .frame(
                    width: box.width * size.width,
                    height: box.height * size.height
                )
                .position(
                    x: box.midX * size.width,
                    y: (1 - box.midY) * size.height // Flip Y axis
                )

            Text(label)
                .font(.caption)
                .padding(2)
                .background(Color.black.opacity(0.6))
                .foregroundColor(color)
                .position(
                    x: box.midX * size.width,
                    y: (1 - box.midY) * size.height - 10
                )
        }
    }
}

// MARK: - Safe Array Access Helper






CameraPreview.swift
FILE: /Users/martingymer/Documents/GolfAIApp/CameraPreview.swift

import SwiftUI
import AVFoundation

struct CameraPreview: UIViewRepresentable {
    let session: AVCaptureSession

    func makeUIView(context: Context) -> UIView {
        let view = UIView(frame: .zero)

        let previewLayer = AVCaptureVideoPreviewLayer(session: session)
        previewLayer.videoGravity = .resizeAspectFill

        // ‚úÖ Set the preview layer‚Äôs connection rotation (iOS 17+ API)
        if let conn = previewLayer.connection,
           conn.isVideoRotationAngleSupported(0) {
            conn.videoRotationAngle = 0   // portrait
        }

        previewLayer.frame = view.bounds
        view.layer.addSublayer(previewLayer)

        // keep layer sized to view
        DispatchQueue.main.async {
            previewLayer.frame = view.bounds
        }

        return view
    }

    func updateUIView(_ uiView: UIView, context: Context) {
        if let previewLayer = uiView.layer.sublayers?.first as? AVCaptureVideoPreviewLayer {
            previewLayer.frame = uiView.bounds
        }
    }
}



CameraService.swift
FILE: /Users/martingymer/Documents/GolfAIApp/CameraService.swift

import Foundation
import AVFoundation
import Vision
import Combine

final class CameraService: NSObject, ObservableObject, AVCaptureVideoDataOutputSampleBufferDelegate {
    private let session = AVCaptureSession()
    private let videoOutput = AVCaptureVideoDataOutput()
    private var input: AVCaptureDeviceInput!
    private var lastPredictionTime = Date.distantPast
    private weak var viewModel: DetectionViewModel?

    public func getSession() -> AVCaptureSession { session }

    func start(viewModel: DetectionViewModel) {
        self.viewModel = viewModel

        guard
            let device = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back),
            let input = try? AVCaptureDeviceInput(device: device)
        else {
            print("‚ùå Could not create AVCaptureDeviceInput")
            return
        }

        self.input = input

        session.beginConfiguration()
        session.sessionPreset = .high

        if session.canAddInput(input) { session.addInput(input) }

        videoOutput.alwaysDiscardsLateVideoFrames = true
        videoOutput.videoSettings = [
            kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA
        ]
        videoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "videoQueue"))

        if session.canAddOutput(videoOutput) { session.addOutput(videoOutput) }

        // Set orientation AFTER adding output
        if let conn = videoOutput.connection(with: .video) {
            if #available(iOS 17.0, *) {
                // iOS 17+: rotation angle in degrees. 0 = portrait
                if conn.isVideoRotationAngleSupported(0) {
                    conn.videoRotationAngle = 0
                }
            } else {
                // iOS 16 and earlier
                if conn.isVideoOrientationSupported {
                    conn.videoOrientation = .portrait
                }
            }
        }



        session.commitConfiguration()
        startSession()
    }

    public func startSession() {
        DispatchQueue.global(qos: .userInitiated).async { self.session.startRunning() }
    }

    public func stop() {
        DispatchQueue.global(qos: .userInitiated).async {
            if self.session.isRunning { self.session.stopRunning() }
        }
    }

    // MARK: - Frame Capture
    func captureOutput(_ output: AVCaptureOutput,
                       didOutput sampleBuffer: CMSampleBuffer,
                       from connection: AVCaptureConnection) {
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        let w = CVPixelBufferGetWidth(pixelBuffer)
        let h = CVPixelBufferGetHeight(pixelBuffer)
        print("üì∑ frame: \(w)x\(h)")

        guard let resizedBuffer = pixelBuffer.resized(to: CGSize(width: 640, height: 640)) else { return }
        let rw = CVPixelBufferGetWidth(resizedBuffer)
        let rh = CVPixelBufferGetHeight(resizedBuffer)
        print("ü™Ñ resized: \(rw)x\(rh)")

        let predictions = YOLOPredictor.shared.predict(pixelBuffer: resizedBuffer)
        print("üß† predictions: \(predictions.count)")


        if !predictions.isEmpty {
            print("Labels seen:", Set(predictions.map { $0.label }))
        }
        DispatchQueue.main.async { [weak self] in
            self?.viewModel?.update(with: predictions)
        }
    }
}



ContentView.swift
FILE: /Users/martingymer/Documents/GolfAIApp/ContentView.swift

import SwiftUI

struct ContentView: View {
    @StateObject var viewModel = DetectionViewModel(predictor: YOLOPredictor.shared)
    @StateObject var cameraService = CameraService()
    // ...



    var body: some View {
        ZStack {
            // Live camera preview
            CameraPreview(session: cameraService.getSession())
                .ignoresSafeArea()

            // Bounding boxes for predictions (with confidence)
            BoundingBoxView(
                boxes:  viewModel.predictions.map { $0.boundingBox },
                labels: viewModel.predictions.map {
                    let pct = Int(($0.confidence as Float) * 100)
                    return "\($0.label) \(pct)%"
                },
                color: .green
            )
            .ignoresSafeArea()

            // Debug overlay text at bottom
            VStack {
                Spacer()
                Text(viewModel.debugMessage)
                    .font(.caption)
                    .foregroundColor(.white)
                    .padding(8)
                    .background(Color.black.opacity(0.6))
                    .cornerRadius(8)
                    .padding(.bottom, 20)
            }
        }
        // üîπ Test Box overlay button (top-left)
        .overlay(
            VStack {
                HStack {
                    Button("‚ñ∂Ô∏è Test Box") {
                        let fake = Prediction(
                            label: "test",
                            confidence: 0.9,
                            boundingBox: CGRect(x: 0.45, y: 0.45, width: 0.10, height: 0.10)
                        )
                        viewModel.update(with: [fake])
                    }
                    .padding(8)
                    .background(Color.black.opacity(0.6))
                    .foregroundColor(.white)
                    .cornerRadius(8)

                    Spacer()
                }
                Spacer()
            }
            .padding(),
            alignment: .topLeading
        )
        .onAppear { cameraService.start(viewModel: viewModel) }
        .onDisappear { cameraService.stop() }
    }
}



DetectionViewModel.swift
FILE: /Users/martingymer/Documents/GolfAIApp/DetectionViewModel.swift

import Combine
import UIKit

@MainActor
final class DetectionViewModel: NSObject, ObservableObject {
    @Published var predictions: [Prediction] = []
    @Published var debugMessage: String = "Awaiting input..."

    private let predictor: Predictor
    private var lastNonEmpty = Date.distantPast
    var holdSeconds: TimeInterval = 0.7

    // No default here anymore
    init(predictor: Predictor) {
        self.predictor = predictor
        super.init()
    }

    func update(with new: [Prediction]) {
        if new.isEmpty {
            if Date().timeIntervalSince(lastNonEmpty) < holdSeconds { return }
            predictions = []
            debugMessage = "‚ö†Ô∏è No predictions"
        } else {
            lastNonEmpty = Date()
            predictions = new
            debugMessage = "‚úÖ \(new.count) object(s) detected"
        }
    }
}



deviceSupportsLiDAR.swift
FILE: /Users/martingymer/Documents/GolfAIApp/deviceSupportsLiDAR.swift

import ARKit

func deviceSupportsLiDAR() -> Bool {
    // Check if the device has a LiDAR sensor
    return ARWorldTrackingConfiguration.supportsFrameSemantics(.sceneDepth)
}



GolfAIApp.swift
FILE: /Users/martingymer/Documents/GolfAIApp/GolfAIApp.swift

import SwiftUI

@main
struct GolfAIApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()   // ‚Üê live camera view
        }
    }
}



GolfAIAppTests:DetectionViewModelTests.swift
FILE: /Users/martingymer/Documents/GolfAIApp/GolfAIAppTests:DetectionViewModelTests.swift

import XCTest
@testable import GolfAIApp

@MainActor
final class DetectionViewModelTests: XCTestCase {

    func test_update_noPredictions_setsWarning() {
        let vm = DetectionViewModel(predictor: FakePredictor(fakeResults: []))
        vm.update(with: [])
        XCTAssertEqual(vm.predictions.count, 0)
        XCTAssertTrue(vm.debugMessage.contains("No predictions"))
    }

    func test_update_onePrediction_setsSuccess() {
        let p = Prediction(label: "golf_ball", confidence: 0.91,
                           boundingBox: .init(x: 0.45, y: 0.55, width: 0.10, height: 0.10))
        let vm = DetectionViewModel(predictor: FakePredictor(fakeResults: [p]))
        vm.update(with: [p])
        XCTAssertEqual(vm.predictions.count, 1)
        XCTAssertTrue(vm.debugMessage.contains("1 object"))
    }
}



GolfAIAppTests:FakePredictor.swift
FILE: /Users/martingymer/Documents/GolfAIApp/GolfAIAppTests:FakePredictor.swift

import CoreVideo
@testable import GolfAIApp

struct FakePredictor: Predictor {
    let fakeResults: [Prediction]
    func predict(pixelBuffer: CVPixelBuffer) -> [Prediction] { fakeResults }
}



LaunchScreenView.swift
FILE: /Users/martingymer/Documents/GolfAIApp/LaunchScreenView.swift

import SwiftUI

struct LaunchScreenView: View {
    @State private var isActive = false

    var body: some View {
        Group {
            if isActive {
                ContentView()  // main app view
            } else {
                VStack(spacing: 20) {
                    Image("test_golf") // from your xcassets
                        .resizable()
                        .scaledToFit()
                        .frame(width: 150, height: 150)

                    Text("Golf Hole Detector")
                        .font(.title)
                        .bold()
                        .foregroundColor(.green)

                    ProgressView()
                        .progressViewStyle(CircularProgressViewStyle())
                }
                .frame(maxWidth: .infinity, maxHeight: .infinity)
                .background(Color.white)
                .onAppear {
                    DispatchQueue.main.asyncAfter(deadline: .now() + 2) {
                        withAnimation {
                            isActive = true
                        }
                    }
                }
            }
        }
    }
}



ModelTest.swift
FILE: /Users/martingymer/Documents/GolfAIApp/ModelTest.swift

// ModelTest.swift ‚Äì leave in project (no test target membership needed)
import SwiftUI
import UIKit

struct ModelTest: View {
    @StateObject private var viewModel: DetectionViewModel

    init() {
        _viewModel = StateObject(wrappedValue: DetectionViewModel(predictor: YOLOPredictor.shared))
    }

    var body: some View {
        VStack {
            if let img = UIImage(named: "test_sample") {
                Image(uiImage: img)
                    .resizable()
                    .scaledToFit()
                    .overlay(
                        BoundingBoxView(
                            boxes: viewModel.predictions.map { $0.boundingBox },
                            labels: viewModel.predictions.map {
                                let pct = Int(($0.confidence as Float) * 100)
                                return "\($0.label) \(pct)%"
                            },
                            color: .red
                        )
                    )
                    .onAppear {
                        let size = CGSize(width: 640, height: 640)
                        if let resized = img.resized(to: size),
                           let pb = resized.toCVPixelBuffer(size: size) {
                            let results = YOLOPredictor.shared.predict(pixelBuffer: pb)
                            viewModel.update(with: results)
                        } else {
                            print("‚ö†Ô∏è Could not create CVPixelBuffer from test image")
                            viewModel.update(with: [])
                        }
                    }
            } else {
                Text("‚ùå test_sample not found")
            }
        }
        .padding()
    }
}



PixelBuffer+Resize.swift
FILE: /Users/martingymer/Documents/GolfAIApp/PixelBuffer+Resize.swift

import CoreVideo
import CoreImage
import UIKit

extension CVPixelBuffer {
    func resized(to size: CGSize) -> CVPixelBuffer? {
        let ciImage = CIImage(cvPixelBuffer: self)
        let scaleX = size.width / CGFloat(CVPixelBufferGetWidth(self))
        let scaleY = size.height / CGFloat(CVPixelBufferGetHeight(self))
        let transform = CGAffineTransform(scaleX: scaleX, y: scaleY)
        let resizedImage = ciImage.transformed(by: transform)

        let context = CIContext()
        var resizedBuffer: CVPixelBuffer?

        let attrs: [CFString: Any] = [
            kCVPixelBufferCGImageCompatibilityKey: true,
            kCVPixelBufferCGBitmapContextCompatibilityKey: true
        ]

        CVPixelBufferCreate(
            kCFAllocatorDefault,
            Int(size.width),
            Int(size.height),
            kCVPixelFormatType_32BGRA,
            attrs as CFDictionary,
            &resizedBuffer
        )

        guard let buffer = resizedBuffer else {
            print("‚ùå Failed to create resized pixel buffer")
            return nil
        }

        context.render(resizedImage, to: buffer)
        return buffer
    }
}




PredictionResult.swift
FILE: /Users/martingymer/Documents/GolfAIApp/PredictionResult.swift

import Foundation
import CoreGraphics

struct Prediction {
    let label: String
    let confidence: Float
    let boundingBox: CGRect
}



Predictor.swift
FILE: /Users/martingymer/Documents/GolfAIApp/Predictor.swift

import CoreVideo

@preconcurrency
protocol Predictor {
    func predict(pixelBuffer: CVPixelBuffer) -> [Prediction]
}



UIImageResize.swift
FILE: /Users/martingymer/Documents/GolfAIApp/UIImageResize.swift

import UIKit
import CoreVideo

extension UIImage {
    
    // Resize UIImage to target size
    func resized(to targetSize: CGSize) -> UIImage? {
        UIGraphicsBeginImageContextWithOptions(targetSize, false, 1.0)
        self.draw(in: CGRect(origin: .zero, size: targetSize))
        let resizedImage = UIGraphicsGetImageFromCurrentImageContext()
        UIGraphicsEndImageContext()
        return resizedImage
    }

    // Convert UIImage to CVPixelBuffer for ML model
    func toCVPixelBuffer(size: CGSize) -> CVPixelBuffer? {
        var pixelBuffer: CVPixelBuffer?
        let attrs: [CFString: Any] = [
            kCVPixelBufferCGImageCompatibilityKey: true,
            kCVPixelBufferCGBitmapContextCompatibilityKey: true
        ]

        let status = CVPixelBufferCreate(
            kCFAllocatorDefault,
            Int(size.width),
            Int(size.height),
            kCVPixelFormatType_32ARGB,
            attrs as CFDictionary,
            &pixelBuffer
        )

        guard status == kCVReturnSuccess, let buffer = pixelBuffer else {
            return nil
        }

        CVPixelBufferLockBaseAddress(buffer, [])
        defer { CVPixelBufferUnlockBaseAddress(buffer, []) }

        guard let context = CGContext(
            data: CVPixelBufferGetBaseAddress(buffer),
            width: Int(size.width),
            height: Int(size.height),
            bitsPerComponent: 8,
            bytesPerRow: CVPixelBufferGetBytesPerRow(buffer),
            space: CGColorSpaceCreateDeviceRGB(),
            bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue
        ) else {
            return nil
        }

        UIGraphicsPushContext(context)
        self.draw(in: CGRect(origin: .zero, size: size))
        UIGraphicsPopContext()

        return buffer
    }
}



YOLOPredictor.swift
FILE: /Users/martingymer/Documents/GolfAIApp/YOLOPredictor.swift

// YOLOPredictor.swift
import Foundation
import Vision
import CoreML
import CoreVideo

/// Runs the GolfHoleDetector.mlpackage via Vision and returns normalized predictions.
final class YOLOPredictor: Predictor {
    static let shared = YOLOPredictor()

    // MARK: - Tunables (start loose, tighten later)
    private let confidenceThreshold: VNConfidence = 0.05        // raise to 0.25‚Äì0.40 once you see hits
    private let allowedLabels: Set<String> = []                  // [] = allow all; set to ["hole"] later
    private let cropMode: VNImageCropAndScaleOption = .scaleFill // try .centerCrop if obsCount stays 0

    private let vnModel: VNCoreMLModel

    private init() {
        do {
            // The class name "GolfHoleDetector" is generated by your .mlpackage
            let mlModel = try GolfHoleDetector(configuration: MLModelConfiguration()).model
            self.vnModel = try VNCoreMLModel(for: mlModel)
        } catch {
            fatalError("‚ùå Failed to load GolfHoleDetector.mlpackage: \(error)")
        }
    }

    func predict(pixelBuffer: CVPixelBuffer) -> [Prediction] {
        var results: [Prediction] = []

        let request = VNCoreMLRequest(model: vnModel) { [confidenceThreshold, allowedLabels] req, error in
            guard error == nil else {
                print("‚ùå Vision request error:", error!)
                return
            }
            guard let obs = req.results as? [VNRecognizedObjectObservation] else {
                print("‚ÑπÔ∏è No recognized-object results in Vision response")
                return
            }

            // Debug: how many raw observations came back?
            print("üîé obsCount:", obs.count)

            for o in obs {
                // Top label (what YOLO thinks it is)
                let top = o.labels.first
                let id  = top?.identifier ?? "unknown"
                let conf = top?.confidence ?? o.confidence

                // More debug: show first few labels per obs
                let tops = o.labels.prefix(3).map { "\($0.identifier)=\(String(format: "%.2f", $0.confidence))" }
                print("  ‚Ü≥", tops.joined(separator: ", "), "box:", o.boundingBox)

                // Filters
                guard conf >= confidenceThreshold else { continue }
                if !allowedLabels.isEmpty && !allowedLabels.contains(id) { continue }

                results.append(Prediction(label: id,
                                          confidence: conf,
                                          boundingBox: o.boundingBox)) // (x,y,w,h) normalized 0..1, Vision coords
            }
        }

        request.imageCropAndScaleOption = cropMode

        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])
        do {
            try handler.perform([request])
        } catch {
            print("‚ùå Vision perform error:", error)
        }

        return results
    }
}



