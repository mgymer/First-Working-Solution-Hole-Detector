BoundingBoxView.swift
FILE: /Users/martingymer/Documents/GolfAIApp/BoundingBoxView.swift

import SwiftUI
struct BoundingBoxView: View {
    let boxes: [CGRect]
    let labels: [String]
    let color: Color

    var body: some View {
        GeometryReader { geometry in
            ForEach(boxes.indices, id: \.self) { index in
                drawBox(
                    boxes[index],
                    label: labels[safe: index] ?? "",
                    in: geometry.size
                )
            }
        }
    }

    @ViewBuilder
    private func drawBox(_ box: CGRect, label: String, in size: CGSize) -> some View {
        ZStack(alignment: .topLeading) {
            Rectangle()
                .stroke(color, lineWidth: 2)
                .frame(
                    width: box.width * size.width,
                    height: box.height * size.height
                )
                .position(
                    x: box.midX * size.width,
                    y: (1 - box.midY) * size.height
                )

            Text(label)
                .font(.caption)
                .padding(2)
                .background(Color.black.opacity(0.6))
                .foregroundColor(color)
                .position(
                    x: box.midX * size.width,
                    y: (1 - box.midY) * size.height - 10
                )
        }
    }
}




CameraPreview.swift
FILE: /Users/martingymer/Documents/GolfAIApp/CameraPreview.swift

import SwiftUI
import AVFoundation

struct CameraPreview: UIViewRepresentable {
    let session: AVCaptureSession

    func makeUIView(context: Context) -> UIView {
        let view = UIView(frame: .zero)

        let previewLayer = AVCaptureVideoPreviewLayer(session: session)
        previewLayer.videoGravity = .resizeAspectFill
        previewLayer.connection?.videoOrientation = .portrait
        previewLayer.frame = view.bounds

        view.layer.addSublayer(previewLayer)

        // Keep layer resized on layout changes
        DispatchQueue.main.async {
            previewLayer.frame = view.bounds
        }

        return view
    }

    func updateUIView(_ uiView: UIView, context: Context) {
        if let previewLayer = uiView.layer.sublayers?.first as? AVCaptureVideoPreviewLayer {
            previewLayer.frame = uiView.bounds
        }
    }
}



CameraService.swift
FILE: /Users/martingymer/Documents/GolfAIApp/CameraService.swift

import AVFoundation
import Vision

class CameraService: NSObject, ObservableObject, AVCaptureVideoDataOutputSampleBufferDelegate {
    private let session = AVCaptureSession()
    private var videoOutput = AVCaptureVideoDataOutput()
    private var input: AVCaptureDeviceInput!
    private var lastPredictionTime = Date.distantPast
    private var viewModel: DetectionViewModel?

    public func getSession() -> AVCaptureSession {
        return session
    }

    func start(viewModel: DetectionViewModel) {
        self.viewModel = viewModel

        guard let device = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back),
              let input = try? AVCaptureDeviceInput(device: device) else {
            print("‚ùå Could not create AVCaptureDeviceInput")
            return
        }

        self.input = input

        session.beginConfiguration()
        session.sessionPreset = .high

        if session.canAddInput(input) {
            session.addInput(input)
        }

        videoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "videoQueue"))
        if session.canAddOutput(videoOutput) {
            session.addOutput(videoOutput)
        }

        session.commitConfiguration()
        startSession()
    }

    /// Start the camera session
    public func startSession() {
        DispatchQueue.global(qos: .userInitiated).async {
            self.session.startRunning()
        }
    }

    /// Stop the camera session ‚Äî this is what ContentView needs!
    public func stop() {
        DispatchQueue.global(qos: .userInitiated).async {
            if self.session.isRunning {
                self.session.stopRunning()
            }
        }
    }

    // MARK: - Frame Capture
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }

        let now = Date()
        guard now.timeIntervalSince(lastPredictionTime) > 0.5 else {
            return // throttle
        }
        lastPredictionTime = now

        print("[DEBUG] Sending frame to predictor")

        if let predictions = YOLOPredictor()?.predict(pixelBuffer: pixelBuffer) {
            print("[DEBUG] Predictions received:")
            for prediction in predictions {
                print("‚Üí \(prediction.label) at \(prediction.boundingBox), confidence: \(prediction.confidence)")
            }

            DispatchQueue.main.async {
                self.viewModel?.update(with: predictions)
            }
        } else {
            print("‚ö†Ô∏è No predictions returned")
        }
    }
}




ContentView.swift
FILE: /Users/martingymer/Documents/GolfAIApp/ContentView.swift

import SwiftUI

struct ContentView: View {
    @StateObject var viewModel = DetectionViewModel()
    @StateObject var cameraService = CameraService()

    var body: some View {
        ZStack {
            // Live camera preview
            CameraPreview(session: cameraService.getSession())
                .ignoresSafeArea()

            // Bounding boxes for predictions
            BoundingBoxView(
                boxes: viewModel.predictions.map { $0.boundingBox },
                labels: viewModel.predictions.map { $0.label },
                color: .green
            )
            .ignoresSafeArea()

            // Debug overlay
            VStack {
                Spacer()
                Text(viewModel.debugMessage)
                    .font(.caption)
                    .foregroundColor(.white)
                    .padding(8)
                    .background(Color.black.opacity(0.6))
                    .cornerRadius(8)
                    .padding(.bottom, 20)
            }
        }
        .onAppear {
            cameraService.start(viewModel: viewModel)
        }
        .onDisappear {
            cameraService.stop()
        }
    }
}



DetectionViewModel.swift
FILE: /Users/martingymer/Documents/GolfAIApp/DetectionViewModel.swift

import Foundation
import Vision
import CoreML
import AVFoundation
import UIKit  // üëà Required for UIImage

class DetectionViewModel: NSObject, ObservableObject {
    @Published var predictions: [Prediction] = []
    @Published var debugMessage: String = "Awaiting input..."

    private let predictor = YOLOPredictor.shared

    // MARK: - Update Predictions
    func update(with predictions: [Prediction]) {
        self.predictions = predictions
        self.debugMessage = predictions.isEmpty
            ? "‚ö†Ô∏è No predictions"
            : "‚úÖ \(predictions.count) object(s) detected"
    }

    // MARK: - Manual Image Prediction (for test image)
    func predict(image: UIImage) {
        guard let resizedImage = image.resized(to: CGSize(width: 416, height: 416)) else {
            print("‚ùå Could not resize image")
            return
        }

        guard let pixelBuffer = resizedImage.toCVPixelBuffer(size: CGSize(width: 416, height: 416)) else {
            print("‚ùå Could not convert image to CVPixelBuffer")
            return
        }

        let results = predictor.predict(pixelBuffer: pixelBuffer)
        DispatchQueue.main.async {
            self.update(with: results)
        }
    }
    }



LaunchScreenView.swift
FILE: /Users/martingymer/Documents/GolfAIApp/LaunchScreenView.swift

import SwiftUI

struct LaunchScreenView: View {
    @State private var isActive = false

    var body: some View {
        Group {
            if isActive {
                ContentView()  // main app view
            } else {
                VStack(spacing: 20) {
                    Image("test_golf") // from your xcassets
                        .resizable()
                        .scaledToFit()
                        .frame(width: 150, height: 150)

                    Text("Golf Hole Detector")
                        .font(.title)
                        .bold()
                        .foregroundColor(.green)

                    ProgressView()
                        .progressViewStyle(CircularProgressViewStyle())
                }
                .frame(maxWidth: .infinity, maxHeight: .infinity)
                .background(Color.white)
                .onAppear {
                    DispatchQueue.main.asyncAfter(deadline: .now() + 2) {
                        withAnimation {
                            isActive = true
                        }
                    }
                }
            }
        }
    }
}



ModelTest.swift
FILE: /Users/martingymer/Documents/GolfAIApp/ModelTest.swift

import SwiftUI

struct ModelTest: View {
    @StateObject private var viewModel = DetectionViewModel()

    var body: some View {
        VStack {
            if let testImage = UIImage(named: "test_sample") {
                Image(uiImage: testImage)
                    .resizable()
                    .scaledToFit()
                    .overlay(
                        BoundingBoxView(
                            boxes: viewModel.boundingBoxes,
                            labels: viewModel.labels,
                            color: .red
                        )
                    )
                    .onAppear {
                        viewModel.predict(image: testImage)
                    }
            } else {
                Text("‚ùå Test image not found.")
            }
        }
        .padding()
    }
}

//I add this



PixelBuffer+Resize.swift
FILE: /Users/martingymer/Documents/GolfAIApp/PixelBuffer+Resize.swift

import CoreVideo
import CoreImage
import UIKit

extension CVPixelBuffer {
    func resized(to size: CGSize) -> CVPixelBuffer? {
        let ciImage = CIImage(cvPixelBuffer: self)
        let scaleX = size.width / CGFloat(CVPixelBufferGetWidth(self))
        let scaleY = size.height / CGFloat(CVPixelBufferGetHeight(self))
        let transform = CGAffineTransform(scaleX: scaleX, y: scaleY)
        let resizedImage = ciImage.transformed(by: transform)

        let context = CIContext()
        var resizedBuffer: CVPixelBuffer?

        let attrs: [CFString: Any] = [
            kCVPixelBufferCGImageCompatibilityKey: true,
            kCVPixelBufferCGBitmapContextCompatibilityKey: true
        ]

        CVPixelBufferCreate(
            kCFAllocatorDefault,
            Int(size.width),
            Int(size.height),
            kCVPixelFormatType_32BGRA,
            attrs as CFDictionary,
            &resizedBuffer
        )

        guard let buffer = resizedBuffer else {
            print("‚ùå Failed to create resized pixel buffer")
            return nil
        }

        context.render(resizedImage, to: buffer)
        return buffer
    }
}




PredictionResult.swift
FILE: /Users/martingymer/Documents/GolfAIApp/PredictionResult.swift

import Foundation
import CoreGraphics

struct Prediction {
    let label: String
    let confidence: Float
    let boundingBox: CGRect
}



UIImageResize.swift
FILE: /Users/martingymer/Documents/GolfAIApp/UIImageResize.swift

import UIKit
import CoreVideo

extension UIImage {
    
    // Resize UIImage to target size
    func resized(to targetSize: CGSize) -> UIImage? {
        UIGraphicsBeginImageContextWithOptions(targetSize, false, 1.0)
        self.draw(in: CGRect(origin: .zero, size: targetSize))
        let resizedImage = UIGraphicsGetImageFromCurrentImageContext()
        UIGraphicsEndImageContext()
        return resizedImage
    }

    // Convert UIImage to CVPixelBuffer for ML model
    func toCVPixelBuffer(size: CGSize) -> CVPixelBuffer? {
        var pixelBuffer: CVPixelBuffer?
        let attrs: [CFString: Any] = [
            kCVPixelBufferCGImageCompatibilityKey: true,
            kCVPixelBufferCGBitmapContextCompatibilityKey: true
        ]

        let status = CVPixelBufferCreate(
            kCFAllocatorDefault,
            Int(size.width),
            Int(size.height),
            kCVPixelFormatType_32ARGB,
            attrs as CFDictionary,
            &pixelBuffer
        )

        guard status == kCVReturnSuccess, let buffer = pixelBuffer else {
            return nil
        }

        CVPixelBufferLockBaseAddress(buffer, [])
        defer { CVPixelBufferUnlockBaseAddress(buffer, []) }

        guard let context = CGContext(
            data: CVPixelBufferGetBaseAddress(buffer),
            width: Int(size.width),
            height: Int(size.height),
            bitsPerComponent: 8,
            bytesPerRow: CVPixelBufferGetBytesPerRow(buffer),
            space: CGColorSpaceCreateDeviceRGB(),
            bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue
        ) else {
            return nil
        }

        UIGraphicsPushContext(context)
        self.draw(in: CGRect(origin: .zero, size: size))
        UIGraphicsPopContext()

        return buffer
    }
}



ViewController.swift
FILE: /Users/martingymer/Documents/GolfAIApp/ViewController.swift

import UIKit
import AVFoundation

class ViewController: UIViewController {
    private var previewLayer: AVCaptureVideoPreviewLayer?
    private let cameraService = CameraService()
    private let predictor = YOLOPredictor.shared
    private let viewModel = DetectionViewModel()

    override func viewDidLoad() {
        super.viewDidLoad()

        // Setup camera preview
        cameraService.setPreviewInView(self.view)

        // Handle frame-by-frame prediction
        cameraService.setBufferHandler { [weak self] buffer in
            guard let self = self else { return }
            let predictions = predictor.predict(pixelBuffer: buffer)
            DispatchQueue.main.async {
                self.viewModel.predictions = predictions
                self.viewModel.debugMessage = predictions.isEmpty ? "‚ö†Ô∏è No predictions" : "‚úÖ \(predictions.count) golf hole(s) detected"
            }
        }

        cameraService.startSession()
    }

    override func viewWillDisappear(_ animated: Bool) {
        super.viewWillDisappear(animated)
        cameraService.stopSession()
    }
}




YOLOPredictor.swift
FILE: /Users/martingymer/Documents/GolfAIApp/YOLOPredictor.swift

import Foundation
import Vision
import CoreML

class YOLOPredictor {
    private let vnModel: VNCoreMLModel
    private let confidenceThreshold: VNConfidence = 0.1

    init?() {
        do {
            let coreMLModel = try GolfHoleDetector(configuration: MLModelConfiguration()).model
            let vnModel = try VNCoreMLModel(for: coreMLModel)
            self.vnModel = vnModel
        } catch {
            print("‚ùå Failed to load model: \(error.localizedDescription)")
            return nil
        }
    }


    func predict(pixelBuffer: CVPixelBuffer) -> [Prediction] {
        var results: [Prediction] = []

        let request = VNCoreMLRequest(model: vnModel) { [self] request, error in
            if let error = error {
                print("‚ö†Ô∏è VNCoreMLRequest failed: \(error.localizedDescription)")
                return
            }

            guard let observations = request.results as? [VNRecognizedObjectObservation] else {
                print("‚ö†Ô∏è Unexpected result type from Vision request")
                return
            }

            for observation in observations where observation.confidence > self.confidenceThreshold {
                let topLabel = observation.labels.first?.identifier ?? "N/A"
                let prediction = Prediction(label: topLabel,
                                            confidence: observation.confidence,
                                            boundingBox: observation.boundingBox)
                results.append(prediction)
            }
        }

        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])
        do {
            try handler.perform([request])
        } catch {
            print("‚ùå Failed to perform request: \(error.localizedDescription)")
        }

        return results
    }
}




